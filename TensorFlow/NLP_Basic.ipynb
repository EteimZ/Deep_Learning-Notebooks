{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-Basic.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMoLrfr8oZubkOLRP4pICU/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EteimZ/Deep_Learning-Notebooks/blob/main/TensorFlow/NLP_Basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awv5dH_H7hT4"
      },
      "source": [
        "# NLP Basics with Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHo6vjtn1I5u"
      },
      "source": [
        "In this notebook I will go through the basic NLP operations like Tokenization and Sequencing using tensorflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHx1N60a1n5_"
      },
      "source": [
        "# Import required Libraries\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4rapHd81cTR"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcSzUCoITror"
      },
      "source": [
        "Tokenization is the process of taking a sentence or a corpus and converting it to integers so every occurance of the word will be given a particular token or number.\n",
        "\n",
        "Keras has a Tokenizer [class](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) for perform tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2mTlDIH2CtX"
      },
      "source": [
        "sentences = ['I love my dog', 'I love my cat', 'You love my Dog!', 'Do you think my dog is amazing?']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PC2j3RLv2TmJ",
        "outputId": "61a1c8ef-57e1-4f78-8711-2fbcd2a51808"
      },
      "source": [
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "print(word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'<OOV>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImNZuyXB3fsv"
      },
      "source": [
        "## Sequencing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNGhGX02WGSD"
      },
      "source": [
        "Tokenization maps each word to a particular integer but we need to convert the sentences to a sequence of integers to perfom any useful NLP operation.\n",
        "\n",
        "Each sentence has to be padded with respect to longest sentences making all the sentences to have the same length.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvQWqBYw4ZjH",
        "outputId": "032bb9ea-81d5-4040-b43e-508d54b1d3d0"
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "padded = pad_sequences(sequences, padding='post') # pad sequences\n",
        "print(padded)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 5  3  2  4  0  0  0]\n",
            " [ 5  3  2  7  0  0  0]\n",
            " [ 6  3  2  4  0  0  0]\n",
            " [ 8  6  9  2  4 10 11]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpYPLBmVY_n9"
      },
      "source": [
        "Now lets test our tokenizer on text it has never seen before, words that it has never seen before will have a integer of 1 why corresponds to the to the Out of Vocobulary token. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiDoTmeT6k99"
      },
      "source": [
        "test_data = ['I really like my dog', 'my dog loves my coat']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6qb5fK37LjY",
        "outputId": "326a084a-c566-4843-abe5-235df7c24015"
      },
      "source": [
        "test_seq = tokenizer.texts_to_sequences(test_data)\n",
        "print(test_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5, 1, 1, 2, 4], [2, 4, 1, 2, 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}